{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d32cb42e",
   "metadata": {},
   "source": [
    "# Hold out Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43defbae",
   "metadata": {},
   "source": [
    "### Configuration and Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d8e2cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as imPipeline\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bebede",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bfec32aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "LargeTrain = pd.read_csv('train.csv')  # Train dataset\n",
    "SmallTest = pd.read_csv('Original_ObesityDataSet.csv')  # Test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c8d5e1",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41f0ca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "LargeTrain = LargeTrain.drop(columns='id')\n",
    "\n",
    "# Extract features and target\n",
    "y_train = LargeTrain['NObeyesdad']\n",
    "X_train = LargeTrain.drop(columns='NObeyesdad')\n",
    "y_test = SmallTest['NObeyesdad']\n",
    "X_test = SmallTest.drop(columns='NObeyesdad')\n",
    "\n",
    "# Split data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numerical_features = ['Age', 'Height', 'Weight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']\n",
    "categorical_features = ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE', 'CALC', 'SCC', 'MTRANS']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f02662a",
   "metadata": {},
   "source": [
    "### Custom Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ce34052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transformers\n",
    "class CustomLabelEncoder(BaseEstimator, TransformerMixin): #Extending BaseEstimator and TransformerMixin classes from sklearn.base\n",
    "    def __init__(self):\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "    def fit(self, y, X=None):\n",
    "        self.label_encoder.fit(y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, y):\n",
    "        return self.label_encoder.transform(y)\n",
    "\n",
    "class CustomScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, numerical_features):\n",
    "        self.numerical_features = numerical_features\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy = self.scaler.transform(X)\n",
    "        return X_copy\n",
    "\n",
    "class CustomTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = pd.DataFrame(X, columns=numerical_features + categorical_features)\n",
    "\n",
    "        # Map categorical features to numerical values\n",
    "        X_copy['family_history_with_overweight'] = X_copy['family_history_with_overweight'].map({'yes': 1, 'no': 0})\n",
    "        X_copy['FAVC'] = X_copy['FAVC'].map({'yes': 1, 'no': 0})\n",
    "        X_copy['SMOKE'] = X_copy['SMOKE'].map({'yes': 1, 'no': 0})\n",
    "        X_copy['SCC'] = X_copy['SCC'].map({'yes': 1, 'no': 0})\n",
    "        X_copy['Gender'] = X_copy['Gender'].map({'Male': 1, 'Female': 0})\n",
    "\n",
    "        custom_mapping = {'no': 1, 'Sometimes': 2, 'Frequently': 3, 'Always': 4}\n",
    "        X_copy['CAEC'] = X_copy['CAEC'].map(custom_mapping)\n",
    "        X_copy['CALC'] = X_copy['CALC'].map(custom_mapping)\n",
    "\n",
    "        one_hot_encoder = OneHotEncoder()\n",
    "        means_of_trns_encoded = pd.DataFrame(\n",
    "            one_hot_encoder.fit_transform(X_copy[['MTRANS']]).toarray(),\n",
    "            columns=one_hot_encoder.get_feature_names_out(['MTRANS'])) \n",
    "        \n",
    "        transformed_df = X_copy.join(means_of_trns_encoded) \n",
    "        transformed_df = transformed_df.drop([\"MTRANS\"], axis=1)\n",
    "        \n",
    "        return transformed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fecf8da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the entire label set (assuming y_train, y_val, and y_test are all available)\n",
    "label_encoder.fit(y_train)\n",
    "\n",
    "# Transform the labels\n",
    "y_train_encoded = label_encoder.transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a85fff",
   "metadata": {},
   "source": [
    "### Model Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5291c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='mean'), numerical_features),\n",
    "        ('cat', SimpleImputer(strategy='most_frequent'), categorical_features)\n",
    "    ])\n",
    "\n",
    "dt_pipeline = imPipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('custom_transformer', CustomTransformer()),\n",
    "        ('custom_scaler', CustomScaler(numerical_features)),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('classifier', DecisionTreeClassifier(max_depth=20, random_state=42))\n",
    "    ])\n",
    "\n",
    "# Define the pipelines with preprocessing and classifier\n",
    "logreg_pipeline = imPipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('custom_transformer', CustomTransformer()),\n",
    "    ('custom_scaler', CustomScaler(numerical_features)),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "svm_pipeline = imPipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('custom_transformer', CustomTransformer()),\n",
    "    ('custom_scaler', CustomScaler(numerical_features)),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', SVC(random_state=42))\n",
    "])\n",
    "\n",
    "rf_pipeline = imPipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('custom_transformer', CustomTransformer()),\n",
    "    ('custom_scaler', CustomScaler(numerical_features)),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=200, max_depth=20, random_state=42))\n",
    "])\n",
    "xgboost_pipeline = imPipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('custom_transformer', CustomTransformer()),\n",
    "        ('custom_scaler', CustomScaler(numerical_features)),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('classifier', XGBClassifier(n_estimators=200, max_depth=10, random_state=42))\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f083608",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2dbf8aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy - Decision Tree: 0.8436897880539499\n",
      "Validation Accuracy - Logistic Regression: 0.8644026974951831\n",
      "Validation Accuracy - SVM: 0.8595857418111753\n",
      "Validation Accuracy - RandomForest: 0.8923410404624278\n",
      "Validation Accuracy - XGBoost: 0.8966763005780347\n"
     ]
    }
   ],
   "source": [
    "# Fit the pipelines on training data\n",
    "logreg_pipeline.fit(X_train, y_train)\n",
    "svm_pipeline.fit(X_train, y_train)\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "dt_pipeline.fit(X_train, y_train)\n",
    "xgboost_pipeline.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Predict on validation data\n",
    "y_val_pred_logreg = logreg_pipeline.predict(X_val)\n",
    "y_val_pred_svm = svm_pipeline.predict(X_val)\n",
    "y_val_pred_rf = rf_pipeline.predict(X_val)\n",
    "y_val_pred_dt = dt_pipeline.predict(X_val)\n",
    "y_val_pred_xgb = xgboost_pipeline.predict(X_val)\n",
    "\n",
    "# Calculate validation accuracies\n",
    "val_accuracy_logreg = accuracy_score(y_val, y_val_pred_logreg)\n",
    "val_accuracy_svm = accuracy_score(y_val, y_val_pred_svm)\n",
    "val_accuracy_rf = accuracy_score(y_val, y_val_pred_rf)\n",
    "val_accuracy_dt = accuracy_score(y_val, y_val_pred_dt)\n",
    "val_accuracy_xgb = accuracy_score(y_val_encoded, y_val_pred_xgb)\n",
    "\n",
    "\n",
    "print(f'Validation Accuracy - Decision Tree: {val_accuracy_dt}')\n",
    "print(f'Validation Accuracy - Logistic Regression: {val_accuracy_logreg}')\n",
    "print(f'Validation Accuracy - SVM: {val_accuracy_svm}')\n",
    "print(f'Validation Accuracy - RandomForest: {val_accuracy_rf}')\n",
    "print(f'Validation Accuracy - XGBoost: {val_accuracy_xgb}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6efe837a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy - Decision Tree: 0.8872572240644244\n",
      "Test Accuracy - Logistic Regression: 0.9062055897678826\n",
      "Test Accuracy - SVM: 0.9047844623401232\n",
      "Test Accuracy - RandomForest: 0.9379441023211748\n",
      "Test Accuracy - XGBoost: 0.9478919943154903\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Predict on test data\n",
    "y_test_pred_logreg = logreg_pipeline.predict(X_test)\n",
    "y_test_pred_svm = svm_pipeline.predict(X_test)\n",
    "y_test_pred_rf = rf_pipeline.predict(X_test)\n",
    "y_test_pred_dt = dt_pipeline.predict(X_test)\n",
    "y_test_pred_xgb = xgboost_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate performance on test data\n",
    "test_accuracy_logreg = accuracy_score(y_test, y_test_pred_logreg)\n",
    "test_accuracy_svm = accuracy_score(y_test, y_test_pred_svm)\n",
    "test_accuracy_rf = accuracy_score(y_test, y_test_pred_rf)\n",
    "test_accuracy_dt = accuracy_score(y_test, y_test_pred_dt)\n",
    "test_accuracy_xgb = accuracy_score(y_test_encoded, y_test_pred_xgb)\n",
    "\n",
    "print(f'Test Accuracy - Decision Tree: {test_accuracy_dt}')\n",
    "print(f'Test Accuracy - Logistic Regression: {test_accuracy_logreg}')\n",
    "print(f'Test Accuracy - SVM: {test_accuracy_svm}')\n",
    "print(f'Test Accuracy - RandomForest: {test_accuracy_rf}')\n",
    "print(f'Test Accuracy - XGBoost: {test_accuracy_xgb}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0976d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix - Logistic Regression:\n",
      "[[270   2   0   0   0   0   0]\n",
      " [ 27 215   0   0   0  43   2]\n",
      " [  0   0 322  18   0   0  11]\n",
      " [  0   0   4 293   0   0   0]\n",
      " [  0   0   1   1 322   0   0]\n",
      " [  0   4   0   0   0 251  35]\n",
      " [  0   0  27   0   0  23 240]]\n",
      "Confusion Matrix - SVM:\n",
      "[[261  11   0   0   0   0   0]\n",
      " [ 17 220   1   0   0  38  11]\n",
      " [  0   3 319  18   0   3   8]\n",
      " [  0   1   2 291   0   3   0]\n",
      " [  0   1   1   1 321   0   0]\n",
      " [  0  16   2   0   0 242  30]\n",
      " [  0   5   9   4   0  16 256]]\n",
      "Confusion Matrix - RandomForest:\n",
      "[[264   8   0   0   0   0   0]\n",
      " [  6 253   0   0   0  23   5]\n",
      " [  0   0 329  14   0   2   6]\n",
      " [  0   1   1 294   1   0   0]\n",
      " [  0   0   1   1 322   0   0]\n",
      " [  0  12   2   0   0 260  16]\n",
      " [  0   4   8   3   0  17 258]]\n",
      "Classification Report - Logistic Regression:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.91      0.99      0.95       272\n",
      "      Normal_Weight       0.97      0.75      0.85       287\n",
      "     Obesity_Type_I       0.91      0.92      0.91       351\n",
      "    Obesity_Type_II       0.94      0.99      0.96       297\n",
      "   Obesity_Type_III       1.00      0.99      1.00       324\n",
      " Overweight_Level_I       0.79      0.87      0.83       290\n",
      "Overweight_Level_II       0.83      0.83      0.83       290\n",
      "\n",
      "           accuracy                           0.91      2111\n",
      "          macro avg       0.91      0.90      0.90      2111\n",
      "       weighted avg       0.91      0.91      0.91      2111\n",
      "\n",
      "Classification Report - SVM:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.94      0.96      0.95       272\n",
      "      Normal_Weight       0.86      0.77      0.81       287\n",
      "     Obesity_Type_I       0.96      0.91      0.93       351\n",
      "    Obesity_Type_II       0.93      0.98      0.95       297\n",
      "   Obesity_Type_III       1.00      0.99      1.00       324\n",
      " Overweight_Level_I       0.80      0.83      0.82       290\n",
      "Overweight_Level_II       0.84      0.88      0.86       290\n",
      "\n",
      "           accuracy                           0.90      2111\n",
      "          macro avg       0.90      0.90      0.90      2111\n",
      "       weighted avg       0.91      0.90      0.90      2111\n",
      "\n",
      "Classification Report - RandomForest:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.98      0.97      0.97       272\n",
      "      Normal_Weight       0.91      0.88      0.90       287\n",
      "     Obesity_Type_I       0.96      0.94      0.95       351\n",
      "    Obesity_Type_II       0.94      0.99      0.97       297\n",
      "   Obesity_Type_III       1.00      0.99      1.00       324\n",
      " Overweight_Level_I       0.86      0.90      0.88       290\n",
      "Overweight_Level_II       0.91      0.89      0.90       290\n",
      "\n",
      "           accuracy                           0.94      2111\n",
      "          macro avg       0.94      0.94      0.94      2111\n",
      "       weighted avg       0.94      0.94      0.94      2111\n",
      "\n",
      "Classification Report - Decision Tree:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.93      0.96      0.94       272\n",
      "      Normal_Weight       0.90      0.76      0.83       287\n",
      "     Obesity_Type_I       0.92      0.86      0.89       351\n",
      "    Obesity_Type_II       0.93      0.98      0.95       297\n",
      "   Obesity_Type_III       1.00      0.99      0.99       324\n",
      " Overweight_Level_I       0.77      0.83      0.80       290\n",
      "Overweight_Level_II       0.76      0.83      0.79       290\n",
      "\n",
      "           accuracy                           0.89      2111\n",
      "          macro avg       0.89      0.89      0.89      2111\n",
      "       weighted avg       0.89      0.89      0.89      2111\n",
      "\n",
      "Classification Report - XGBoost:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.98      0.97      0.97       272\n",
      "      Normal_Weight       0.91      0.88      0.90       287\n",
      "     Obesity_Type_I       0.96      0.94      0.95       351\n",
      "    Obesity_Type_II       0.94      0.99      0.97       297\n",
      "   Obesity_Type_III       1.00      0.99      1.00       324\n",
      " Overweight_Level_I       0.86      0.90      0.88       290\n",
      "Overweight_Level_II       0.91      0.89      0.90       290\n",
      "\n",
      "           accuracy                           0.94      2111\n",
      "          macro avg       0.94      0.94      0.94      2111\n",
      "       weighted avg       0.94      0.94      0.94      2111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate confusion matrices\n",
    "conf_matrix_logreg = confusion_matrix(y_test, y_test_pred_logreg)\n",
    "conf_matrix_svm = confusion_matrix(y_test, y_test_pred_svm)\n",
    "conf_matrix_rf = confusion_matrix(y_test, y_test_pred_rf)\n",
    "\n",
    "print('Confusion Matrix - Logistic Regression:')\n",
    "print(conf_matrix_logreg)\n",
    "print('Confusion Matrix - SVM:')\n",
    "print(conf_matrix_svm)\n",
    "print('Confusion Matrix - RandomForest:')\n",
    "print(conf_matrix_rf)\n",
    "\n",
    "# Generate classification reports\n",
    "class_report_logreg = classification_report(y_test, y_test_pred_logreg)\n",
    "class_report_svm = classification_report(y_test, y_test_pred_svm)\n",
    "class_report_rf = classification_report(y_test, y_test_pred_rf)\n",
    "class_report_dt = classification_report(y_test, y_test_pred_dt)\n",
    "class_report_xgb = classification_report(y_test_encoded, y_test_pred_xgb)\n",
    "\n",
    "print('Classification Report - Logistic Regression:')\n",
    "print(class_report_logreg)\n",
    "print('Classification Report - SVM:')\n",
    "print(class_report_svm)\n",
    "print('Classification Report - RandomForest:')\n",
    "print(class_report_rf)\n",
    "\n",
    "print('Classification Report - Decision Tree:')\n",
    "print(class_report_dt)\n",
    "\n",
    "print('Classification Report - XGBoost:')\n",
    "print(class_report_rf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
