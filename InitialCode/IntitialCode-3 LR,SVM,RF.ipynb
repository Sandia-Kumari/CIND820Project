{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d32cb42e",
   "metadata": {},
   "source": [
    "# Approach 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065df754",
   "metadata": {},
   "source": [
    "## Using large dataset for training and small dataset for testing and doing train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43defbae",
   "metadata": {},
   "source": [
    "### Configuration and Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d8e2cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bebede",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfec32aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "LargeTrain = pd.read_csv('train.csv')  # Original dataset\n",
    "SmallTest = pd.read_csv('Original_ObesityDataSet.csv')  # Test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c8d5e1",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41f0ca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "LargeTrain = LargeTrain.drop(columns='id')\n",
    "\n",
    "# Extract features and target\n",
    "y_train = LargeTrain['NObeyesdad']\n",
    "X_train = LargeTrain.drop(columns='NObeyesdad')\n",
    "y_test = SmallTest['NObeyesdad']\n",
    "X_test = SmallTest.drop(columns='NObeyesdad')\n",
    "\n",
    "# Split data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numerical_features = ['Age', 'Height', 'Weight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']\n",
    "categorical_features = ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE', 'CALC', 'SCC', 'MTRANS']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f02662a",
   "metadata": {},
   "source": [
    "### Custom Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce34052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transformers\n",
    "class CustomLabelEncoder(BaseEstimator, TransformerMixin): #Extending BaseEstimator and TransformerMixin classes from sklearn.base\n",
    "    def __init__(self):\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "    def fit(self, y, X=None):\n",
    "        self.label_encoder.fit(y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, y):\n",
    "        return self.label_encoder.transform(y)\n",
    "\n",
    "class CustomScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, numerical_features):\n",
    "        self.numerical_features = numerical_features\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy = self.scaler.transform(X)\n",
    "        return X_copy\n",
    "\n",
    "class CustomTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = pd.DataFrame(X, columns=numerical_features + categorical_features)\n",
    "\n",
    "        # Map categorical features to numerical values\n",
    "        X_copy['family_history_with_overweight'] = X_copy['family_history_with_overweight'].map({'yes': 1, 'no': 0})\n",
    "        X_copy['FAVC'] = X_copy['FAVC'].map({'yes': 1, 'no': 0})\n",
    "        X_copy['SMOKE'] = X_copy['SMOKE'].map({'yes': 1, 'no': 0})\n",
    "        X_copy['SCC'] = X_copy['SCC'].map({'yes': 1, 'no': 0})\n",
    "        X_copy['Gender'] = X_copy['Gender'].map({'Male': 1, 'Female': 0})\n",
    "\n",
    "        custom_mapping = {'no': 1, 'Sometimes': 2, 'Frequently': 3, 'Always': 4}\n",
    "        X_copy['CAEC'] = X_copy['CAEC'].map(custom_mapping)\n",
    "        X_copy['CALC'] = X_copy['CALC'].map(custom_mapping)\n",
    "\n",
    "        one_hot_encoder = OneHotEncoder()\n",
    "        means_of_trns_encoded = pd.DataFrame(\n",
    "            one_hot_encoder.fit_transform(X_copy[['MTRANS']]).toarray(),\n",
    "            columns=one_hot_encoder.get_feature_names_out(['MTRANS'])) \n",
    "        \n",
    "        transformed_df = X_copy.join(means_of_trns_encoded) \n",
    "        transformed_df = transformed_df.drop([\"MTRANS\"], axis=1)\n",
    "        \n",
    "        return transformed_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a85fff",
   "metadata": {},
   "source": [
    "### Model Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5291c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='mean'), numerical_features),\n",
    "        ('cat', SimpleImputer(strategy='most_frequent'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define the pipelines with preprocessing and classifier\n",
    "logreg_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('custom_transformer', CustomTransformer()),\n",
    "    ('custom_scaler', CustomScaler(numerical_features)),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "svm_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('custom_transformer', CustomTransformer()),\n",
    "    ('custom_scaler', CustomScaler(numerical_features)),\n",
    "    ('classifier', SVC(random_state=42))\n",
    "])\n",
    "\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('custom_transformer', CustomTransformer()),\n",
    "    ('custom_scaler', CustomScaler(numerical_features)),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f083608",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6efe837a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy - Logistic Regression: 0.8622350674373795\n",
      "Validation Accuracy - SVM: 0.8562138728323699\n",
      "Validation Accuracy - RandomForest: 0.8942678227360308\n",
      "Test Accuracy - Logistic Regression: 0.9024159166271909\n",
      "Test Accuracy - SVM: 0.8995736617716722\n",
      "Test Accuracy - RandomForest: 0.9341544291804832\n",
      "Confusion Matrix - Logistic Regression:\n",
      "[[270   2   0   0   0   0   0]\n",
      " [ 25 229   0   0   0  30   3]\n",
      " [  0   0 318  22   0   0  11]\n",
      " [  0   0   3 294   0   0   0]\n",
      " [  0   0   1   1 322   0   0]\n",
      " [  0   8   0   0   0 241  41]\n",
      " [  0   0  33   4   0  22 231]]\n",
      "Confusion Matrix - SVM:\n",
      "[[261  11   0   0   0   0   0]\n",
      " [ 15 223   1   0   0  33  15]\n",
      " [  0   4 315  22   0   5   5]\n",
      " [  0   1   2 291   0   3   0]\n",
      " [  0   1   1   1 321   0   0]\n",
      " [  0  20   2   0   0 231  37]\n",
      " [  0   7   9   4   0  13 257]]\n",
      "Confusion Matrix - RandomForest:\n",
      "[[264   8   0   0   0   0   0]\n",
      " [  6 255   0   0   0  23   3]\n",
      " [  0   0 330  14   0   1   6]\n",
      " [  0   1   2 294   0   0   0]\n",
      " [  0   0   2   0 322   0   0]\n",
      " [  1  11   2   0   0 249  27]\n",
      " [  0   4   7   5   0  16 258]]\n",
      "Classification Report - Logistic Regression:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.92      0.99      0.95       272\n",
      "      Normal_Weight       0.96      0.80      0.87       287\n",
      "     Obesity_Type_I       0.90      0.91      0.90       351\n",
      "    Obesity_Type_II       0.92      0.99      0.95       297\n",
      "   Obesity_Type_III       1.00      0.99      1.00       324\n",
      " Overweight_Level_I       0.82      0.83      0.83       290\n",
      "Overweight_Level_II       0.81      0.80      0.80       290\n",
      "\n",
      "           accuracy                           0.90      2111\n",
      "          macro avg       0.90      0.90      0.90      2111\n",
      "       weighted avg       0.90      0.90      0.90      2111\n",
      "\n",
      "Classification Report - SVM:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.95      0.96      0.95       272\n",
      "      Normal_Weight       0.84      0.78      0.81       287\n",
      "     Obesity_Type_I       0.95      0.90      0.93       351\n",
      "    Obesity_Type_II       0.92      0.98      0.95       297\n",
      "   Obesity_Type_III       1.00      0.99      1.00       324\n",
      " Overweight_Level_I       0.81      0.80      0.80       290\n",
      "Overweight_Level_II       0.82      0.89      0.85       290\n",
      "\n",
      "           accuracy                           0.90      2111\n",
      "          macro avg       0.90      0.90      0.90      2111\n",
      "       weighted avg       0.90      0.90      0.90      2111\n",
      "\n",
      "Classification Report - RandomForest:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Insufficient_Weight       0.97      0.97      0.97       272\n",
      "      Normal_Weight       0.91      0.89      0.90       287\n",
      "     Obesity_Type_I       0.96      0.94      0.95       351\n",
      "    Obesity_Type_II       0.94      0.99      0.96       297\n",
      "   Obesity_Type_III       1.00      0.99      1.00       324\n",
      " Overweight_Level_I       0.86      0.86      0.86       290\n",
      "Overweight_Level_II       0.88      0.89      0.88       290\n",
      "\n",
      "           accuracy                           0.93      2111\n",
      "          macro avg       0.93      0.93      0.93      2111\n",
      "       weighted avg       0.93      0.93      0.93      2111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the pipelines on training data\n",
    "logreg_pipeline.fit(X_train, y_train)\n",
    "svm_pipeline.fit(X_train, y_train)\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation data\n",
    "y_val_pred_logreg = logreg_pipeline.predict(X_val)\n",
    "y_val_pred_svm = svm_pipeline.predict(X_val)\n",
    "y_val_pred_rf = rf_pipeline.predict(X_val)\n",
    "\n",
    "# Calculate validation accuracies\n",
    "val_accuracy_logreg = accuracy_score(y_val, y_val_pred_logreg)\n",
    "val_accuracy_svm = accuracy_score(y_val, y_val_pred_svm)\n",
    "val_accuracy_rf = accuracy_score(y_val, y_val_pred_rf)\n",
    "\n",
    "print(f'Validation Accuracy - Logistic Regression: {val_accuracy_logreg}')\n",
    "print(f'Validation Accuracy - SVM: {val_accuracy_svm}')\n",
    "print(f'Validation Accuracy - RandomForest: {val_accuracy_rf}')\n",
    "\n",
    "# Predict on test data\n",
    "y_test_pred_logreg = logreg_pipeline.predict(X_test)\n",
    "y_test_pred_svm = svm_pipeline.predict(X_test)\n",
    "y_test_pred_rf = rf_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate performance on test data\n",
    "test_accuracy_logreg = accuracy_score(y_test, y_test_pred_logreg)\n",
    "test_accuracy_svm = accuracy_score(y_test, y_test_pred_svm)\n",
    "test_accuracy_rf = accuracy_score(y_test, y_test_pred_rf)\n",
    "\n",
    "print(f'Test Accuracy - Logistic Regression: {test_accuracy_logreg}')\n",
    "print(f'Test Accuracy - SVM: {test_accuracy_svm}')\n",
    "print(f'Test Accuracy - RandomForest: {test_accuracy_rf}')\n",
    "\n",
    "# Generate confusion matrices\n",
    "conf_matrix_logreg = confusion_matrix(y_test, y_test_pred_logreg)\n",
    "conf_matrix_svm = confusion_matrix(y_test, y_test_pred_svm)\n",
    "conf_matrix_rf = confusion_matrix(y_test, y_test_pred_rf)\n",
    "\n",
    "print('Confusion Matrix - Logistic Regression:')\n",
    "print(conf_matrix_logreg)\n",
    "print('Confusion Matrix - SVM:')\n",
    "print(conf_matrix_svm)\n",
    "print('Confusion Matrix - RandomForest:')\n",
    "print(conf_matrix_rf)\n",
    "\n",
    "# Generate classification reports\n",
    "class_report_logreg = classification_report(y_test, y_test_pred_logreg)\n",
    "class_report_svm = classification_report(y_test, y_test_pred_svm)\n",
    "class_report_rf = classification_report(y_test, y_test_pred_rf)\n",
    "\n",
    "print('Classification Report - Logistic Regression:')\n",
    "print(class_report_logreg)\n",
    "print('Classification Report - SVM:')\n",
    "print(class_report_svm)\n",
    "print('Classification Report - RandomForest:')\n",
    "print(class_report_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83f2276a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "TP: {'Insufficient_Weight': 270, 'Normal_Weight': 229, 'Obesity_Type_I': 318, 'Obesity_Type_II': 294, 'Obesity_Type_III': 322, 'Overweight_Level_I': 241, 'Overweight_Level_II': 231} FP: {'Insufficient_Weight': 25, 'Normal_Weight': 10, 'Obesity_Type_I': 37, 'Obesity_Type_II': 27, 'Obesity_Type_III': 0, 'Overweight_Level_I': 52, 'Overweight_Level_II': 55} TN: {'Insufficient_Weight': 1814, 'Normal_Weight': 1814, 'Obesity_Type_I': 1723, 'Obesity_Type_II': 1787, 'Obesity_Type_III': 1787, 'Overweight_Level_I': 1769, 'Overweight_Level_II': 1766} FN: {'Insufficient_Weight': 2, 'Normal_Weight': 58, 'Obesity_Type_I': 33, 'Obesity_Type_II': 3, 'Obesity_Type_III': 2, 'Overweight_Level_I': 49, 'Overweight_Level_II': 59}\n",
      "\n",
      "Support Vector Machine:\n",
      "TP: {'Insufficient_Weight': 261, 'Normal_Weight': 223, 'Obesity_Type_I': 315, 'Obesity_Type_II': 291, 'Obesity_Type_III': 321, 'Overweight_Level_I': 231, 'Overweight_Level_II': 257} FP: {'Insufficient_Weight': 15, 'Normal_Weight': 44, 'Obesity_Type_I': 15, 'Obesity_Type_II': 27, 'Obesity_Type_III': 0, 'Overweight_Level_I': 54, 'Overweight_Level_II': 57} TN: {'Insufficient_Weight': 1824, 'Normal_Weight': 1780, 'Obesity_Type_I': 1745, 'Obesity_Type_II': 1787, 'Obesity_Type_III': 1787, 'Overweight_Level_I': 1767, 'Overweight_Level_II': 1764} FN: {'Insufficient_Weight': 11, 'Normal_Weight': 64, 'Obesity_Type_I': 36, 'Obesity_Type_II': 6, 'Obesity_Type_III': 3, 'Overweight_Level_I': 59, 'Overweight_Level_II': 33}\n",
      "\n",
      "Random Forest:\n",
      "TP: {'Insufficient_Weight': 264, 'Normal_Weight': 255, 'Obesity_Type_I': 330, 'Obesity_Type_II': 294, 'Obesity_Type_III': 322, 'Overweight_Level_I': 249, 'Overweight_Level_II': 258} FP: {'Insufficient_Weight': 7, 'Normal_Weight': 24, 'Obesity_Type_I': 13, 'Obesity_Type_II': 19, 'Obesity_Type_III': 0, 'Overweight_Level_I': 40, 'Overweight_Level_II': 36} TN: {'Insufficient_Weight': 1832, 'Normal_Weight': 1800, 'Obesity_Type_I': 1747, 'Obesity_Type_II': 1795, 'Obesity_Type_III': 1787, 'Overweight_Level_I': 1781, 'Overweight_Level_II': 1785} FN: {'Insufficient_Weight': 8, 'Normal_Weight': 32, 'Obesity_Type_I': 21, 'Obesity_Type_II': 3, 'Obesity_Type_III': 2, 'Overweight_Level_I': 41, 'Overweight_Level_II': 32}\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate TP, FP, TN, FN for each class\n",
    "def calculate_class_metrics(y_test, y_pred, classes):\n",
    "    TP = {cls: np.sum((y_test == cls) & (y_pred == cls)) for cls in classes}\n",
    "    FP = {cls: np.sum((y_test != cls) & (y_pred == cls)) for cls in classes}\n",
    "    TN = {cls: np.sum((y_test != cls) & (y_pred != cls)) for cls in classes}\n",
    "    FN = {cls: np.sum((y_test == cls) & (y_pred != cls)) for cls in classes}\n",
    "\n",
    "    return TP, FP, TN, FN\n",
    "\n",
    "# Get unique classes\n",
    "classes = np.unique(y_test)\n",
    "\n",
    "# Calculate metrics for each model\n",
    "print(\"Logistic Regression:\")\n",
    "TP_logreg, FP_logreg, TN_logreg, FN_logreg = calculate_class_metrics(y_test, y_test_pred_logreg, classes)\n",
    "print(\"TP:\", TP_logreg, \"FP:\", FP_logreg, \"TN:\", TN_logreg, \"FN:\", FN_logreg)\n",
    "\n",
    "print(\"\\nSupport Vector Machine:\")\n",
    "TP_svm, FP_svm, TN_svm, FN_svm = calculate_class_metrics(y_test, y_test_pred_svm, classes)\n",
    "print(\"TP:\", TP_svm, \"FP:\", FP_svm, \"TN:\", TN_svm, \"FN:\", FN_svm)\n",
    "\n",
    "print(\"\\nRandom Forest:\")\n",
    "TP_rf, FP_rf, TN_rf, FN_rf = calculate_class_metrics(y_test, y_test_pred_rf, classes)\n",
    "print(\"TP:\", TP_rf, \"FP:\", FP_rf, \"TN:\", TN_rf, \"FN:\", FN_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86743a73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
